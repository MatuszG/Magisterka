{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bca475cb-e91f-4654-b662-e3d8141398d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -q -U bitsandbytes\n",
    "# !pip install -q -U git+https://github.com/huggingface/transformers.git \n",
    "# !pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "# !pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "# !pip install bitsandbytes // valid\n",
    "# !pip install transformers\n",
    "# !pip install peft accelerate\n",
    "# !pip install -q datasets\n",
    "# !pip install --upgrade pyarrow\n",
    "# !pip install pyarrow==15.0.2\n",
    "# model_id = \"EleutherAI/gpt-neo-1.3B\"\n",
    "# !pip install --upgrade pyarrow\n",
    "# !pip install datasets==2.12.0 transformers==4.28.0\n",
    "# !pip install peft accelerate\n",
    "# !pip install -q datasets\n",
    "# !pip install transformers[torch]\n",
    "# !pip install torch --upgrade\n",
    "# !pip install transformers --upgrade\n",
    "# !pip install pyarrow==5.0.0\n",
    "# !pip install tensorflow\n",
    "# !pip install tf-keras\n",
    "# !pip install peft\n",
    "# !pip install transformers\n",
    "# !pip install wandb\n",
    "# !pip install transformers[torch]\n",
    "# !pip install torch==2.0.0+cu118 torchvision==0.15.0+cu118 torchaudio==2.0.0+cu118 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "model_id = \"gpt2-large\"\n",
    "model_name = \"gpt2-large\"\n",
    "\n",
    "import wandb\n",
    "import os\n",
    "os.environ[\"WANDB_PROJECT\"]=model_name\n",
    "os.environ[\"WANDB_LOG_MODEL\"]=\"true\"\n",
    "os.environ[\"WANDB_WATCH\"]=\"false\"\n",
    "os.environ[\"WANDB_NAME\"]=\"first\"\n",
    "os.environ[\"WANDB_API_KEY\"] = \"b689f7c91f1ec7520fa8da927f175f1efd587181\"\n",
    "# wandb.login(key=\"b689f7c91f1ec7520fa8da927f175f1efd587181\")\n",
    "import torch\n",
    "# import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoModel, TextDataset, DataCollatorForLanguageModeling, TrainingArguments, GPT2LMHeadModel ## do poprawki\n",
    "from transformers import Trainer\n",
    "# from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "# from datasets import load_dataset\n",
    "import gc\n",
    "import torch\n",
    "# import pyarrow.parquet as pq\n",
    "\n",
    "\n",
    "default_path = 'Magisterka'\n",
    "# data_path_output = os.path.join(default_path,\"EleutherAI\", model_name)\n",
    "data_path_output = os.path.join(default_path, model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc8febdb-aa5a-4734-8fd7-424db853a9eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "765e14fb2a6446d7b8b998119e8b3aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8dfb861b9384ee6b71c6443e3d13dc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "737d8e5e1f824fed879775784b84c677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2039adc3d83b44ad9b2de53f481a1279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "337ad74ccf2040d2bc226aaed4f115a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.data.datasets.language_modeling.TextDataset object at 0x7f8f4ef8c8b0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# !pip install datasets --upgrade\n",
    "import os\n",
    "from transformers import AutoTokenizer, GPTNeoForCausalLM\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "# model_name = model_id\n",
    "frac_value = 1\n",
    "# Load a pre-trained tokenizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(data_path_output)\n",
    "except:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.bos_token = \"<|startoftext|>\"\n",
    "tokenizer.save_pretrained(data_path_output)\n",
    "\n",
    "path_wow_quests = os.path.join(\"data.csv\")\n",
    "path_wow_quests_save_train = os.path.join(\"data_train.txt\")\n",
    "path_wow_quests_save_validation = os.path.join(\"data_validation.txt\")\n",
    "\n",
    "text_data_train = []\n",
    "text_data_valid = []\n",
    "data = pd.read_csv(path_wow_quests)\n",
    "data = data.dropna()\n",
    "data = data.dropna(subset=['Quest Description'])\n",
    "data = data.sample(frac=frac_value, random_state=42).reset_index(drop=True)\n",
    "train_ratio = 0.9\n",
    "eval_ratio = 1 - train_ratio\n",
    "\n",
    "num_train_samples = int(train_ratio * len(data))\n",
    "num_eval_samples = len(data) - num_train_samples\n",
    "train_data = data.iloc[:num_train_samples]\n",
    "eval_data = data.iloc[num_train_samples:]\n",
    "\n",
    "eos_token = tokenizer.eos_token\n",
    "sos_token = tokenizer.bos_token\n",
    "for i, row in train_data.iterrows():\n",
    "    descrp_org = str(row[\"Quest Description\"])\n",
    "    data_to_save = sos_token + \"Title: \" + str(row[\"Quest Title\"]) + \" Description: \" + descrp_org + eos_token\n",
    "    data_to_save = data_to_save.replace('\\n', '')\n",
    "    text_data_train.append(data_to_save)\n",
    "\n",
    "for i, row in eval_data.iterrows():\n",
    "    descrp_org = str(row[\"Quest Description\"])\n",
    "    data_to_save = sos_token + \"Title: \" + str(row[\"Quest Title\"]) + \" Description: \" + descrp_org + eos_token\n",
    "    data_to_save = data_to_save.replace('\\n', '')\n",
    "    text_data_valid.append(data_to_save)\n",
    "\n",
    "with open(path_wow_quests_save_train, \"w\", newline=\"\\n\") as file:\n",
    "    for text in text_data_train:\n",
    "      file.write(text)\n",
    "\n",
    "with open(path_wow_quests_save_validation, \"w\", newline=\"\\n\") as file:\n",
    "    for text in text_data_valid:\n",
    "      file.write(text)\n",
    "\n",
    "def load_text_data(data):\n",
    "    return pd.DataFrame(data, columns=[\"text\"])\n",
    "\n",
    "\n",
    "train_df = load_text_data(text_data_train)\n",
    "validation_df = load_text_data(text_data_valid)\n",
    "\n",
    "# print(train_df[\"text\"])\n",
    "# Convert the pandas DataFrame to a Hugging Face Dataset\n",
    "# train_dataset = Dataset.from_pandas(train_df)\n",
    "# validation_dataset = Dataset.from_pandas(validation_df)\n",
    "\n",
    "# # Define a tokenization function\n",
    "# def tokenize_function(examples):\n",
    "#     return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "\n",
    "# def find_tokenizer_length():\n",
    "#     max_tokenizer_length = 0\n",
    "#     eos_token_id = 50256\n",
    "#     for row in train_dataset:\n",
    "#         print(row)\n",
    "#         tokens = tokenize_function(row).count(eos_token_id)\n",
    "#         if tokens > max_tokenizer_length:\n",
    "#             max_tokenizer_length = tokens\n",
    "#     for row in validation_dataset:\n",
    "#         tokens = tokenize_function(row).count(eos_token_id)\n",
    "#         if tokens > max_tokenizer_length:\n",
    "#             max_tokenizer_length = tokens\n",
    "#     print(max_tokenizer_length)\n",
    "    \n",
    "# find_tokenizer_length()\n",
    "\n",
    "# Apply the tokenization function to the datasets\n",
    "# train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "# validation_dataset = validation_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "# validation_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "train_dataset = TextDataset(tokenizer=tokenizer, file_path=path_wow_quests_save_train, block_size=64)\n",
    "validation_dataset = TextDataset(tokenizer=tokenizer, file_path=path_wow_quests_save_validation, block_size=64)\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "# validation_dataloader = DataLoader(validation_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "\n",
    "# tokenizer(batch_sentences, padding='max_length', truncation=True)\n",
    "\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b7451bd2-6963-480f-9b5e-11c41451e538",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_dataset = TextDataset(tokenizer=tokenizer, file_path=\"data_train.txt\", block_size=64)\n",
    "# validation_dataset = TextDataset(tokenizer=tokenizer, file_path=\"data_validation.txt\", block_size=64)\n",
    "from torch.utils.data import DataLoader\n",
    "from accelerate import Accelerator\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "def fine_tune_gpt2(model_id, output_dir):\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"CUDA is available. Using GPU: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"CUDA is not available. Using CPU.\")\n",
    "    model = None\n",
    "    gc.collect()\n",
    "    try:\n",
    "        torch.cuda.empty_cache()\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Caught a runtime error: {e}\")\n",
    "        \n",
    "    try:\n",
    "        model = GPT2LMHeadModel.from_pretrained(output_dir) #GPT2LMHeadModel||GPTNeoForCausalLM\n",
    "    except:\n",
    "        model = GPT2LMHeadModel.from_pretrained(model_id) #model_id\n",
    "    # print(device)\n",
    "    # model = model.to(\"cpu\")\n",
    "    # train_dataloader = DataLoader(train_dataset, batch_size=12, shuffle=True, pin_memory=True)\n",
    "    # eval_dataloader = DataLoader(validation_dataset, batch_size=12, shuffle=False, pin_memory=True)\n",
    "    # accelerator = Accelerator()\n",
    "    # model, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    #     model, train_dataloader, eval_dataloader\n",
    "    # )\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "    # Set training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=2,\n",
    "        report_to=\"wandb\",\n",
    "        # eval_strategy='epoch',\n",
    "        evaluation_strategy='epoch',\n",
    "        save_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=24, #8/24\n",
    "        per_device_eval_batch_size=24, #8/24\n",
    "        logging_dir= os.path.join(data_path_output, '/logs'),\n",
    "        logging_steps=100,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True\n",
    "    )\n",
    "    # Train the model\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=validation_dataset,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],  # stop after 3 non-improving epochs\n",
    "    )\n",
    "    trainer.train()\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    \n",
    "    # Token indices sequence length is longer than the specified maximum sequence length for this model (244752 > 1024). Running this sequence through the model will result in indexing errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f460506-3377-4b03-b6d4-4f8433fc15f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU: NVIDIA L4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='822' max='822' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [822/822 21:43, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.739000</td>\n",
       "      <td>2.843668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.165300</td>\n",
       "      <td>2.907673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    }
   ],
   "source": [
    " # Make only GPU 0 visible\n",
    "fine_tune_gpt2(model_id, data_path_output)\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8000d7cc-85f3-4997-a6f0-dcc99ef0b93a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#zrobiÄ‡ skrypt do odpalania modeli, generowania i porÃ³wnywania z datasetem - zapisywane do pliku\n",
    "# ocenic ktory sposob jest lepszy na trenowanie modelu (szybkosc i jakosc)\n",
    "# odpalic model z kwantyzacjÄ…\n",
    "# trenowac model z kwantyzacja\n",
    "# znalezc nowe 2 zbiory danych\n",
    "# wymyslec i zbudowac logike generowania i walidowania questow (najlepiej w oparciu o jakies zewnetrzne api wiekszego modelu) np. llama2/3\n",
    "\n",
    "#1. \n",
    "    # odpaliÄ‡ na lepszym gpu..\n",
    "    # sprawdziÄ‡ odpalanie na kilku gpu (poÅ‚Ä…czyÄ‡ im memory)\n",
    "#2.\n",
    "    # odpaliÄ‡ model z kwantyzacjÄ…\n",
    "#3.\n",
    "    # wygenerowaÄ‡ prompt z gpt4o lub gemini odnoÅ›nie Å›wiata WoW\n",
    "        # zobaczyÄ‡ api do gemini lub llama lub innego modelu\n",
    "        # odpaliÄ‡ model neo i w nim wklepaÄ‡ prompt\n",
    "#4. # zrobiÄ‡ kod do modelu \n",
    "    # odpalamy wytrenowany model\n",
    "    # generujemy questy\n",
    "    # (sprawdzamy, czy jest juÅ¼ taki quest)\n",
    "    # sprawdzamy drugim modelem czy quest jest git\n",
    "    # zadajemy pytania do tego questa\n",
    "    # odpowiadamy na nie i sprawdzamy czy na pewno jest git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7b1a29-b841-4109-8862-f2ded73d4906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zrobiÄ‡ notatnik z gpt-neo..\n",
    "# zrobiÄ‡ notatnik z modelem kwantyzowanym\n",
    "# sprÃ³bowaÄ‡ zapytania z api\n",
    "# zrobiÄ‡ vector database..\n",
    "# znalezc 2 dodatkowe zbiory danych\n",
    "# wydzielic pliki do tworzenia danych i trenowania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a78c4988-bd87-46ad-9505-7330a263edd0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.1+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "cuDNN version: 8902\n",
      "Device count: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"cuDNN version: {torch.backends.cudnn.version()}\")\n",
    "print(f\"Device count: {torch.cuda.device_count()}\")\n",
    "\n",
    "# PyTorch version: 2.0.0+cu118\n",
    "# CUDA available: True\n",
    "# CUDA version: 11.8\n",
    "# cuDNN version: 8900\n",
    "# Device count: 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b54b957-5b7b-42b3-a6da-dfed62dedd49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip list\n",
    "input_tensor = torch.randn(5, 10).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1db07b2f-df33-4909-b707-0f4aff6f4a01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Training Epoch 1:   0%|          | 0/1233 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 17\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m batch  \u001b[38;5;66;03m# Your dataset should return tokenized inputs and labels\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     19\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "try:\n",
    "    model = GPT2LMHeadModel.from_pretrained(output_dir) #GPT2LMHeadModel||GPTNeoForCausalLM\n",
    "except:\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_id) #model_id\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch + 1}\"):\n",
    "        inputs, labels = batch  # Your dataset should return tokenized inputs and labels\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Average Training Loss: {avg_train_loss}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(validation_dataloader, desc=\"Validation\"):\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(validation_dataloader)\n",
    "    print(f\"Average Validation Loss: {avg_val_loss}\")\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu121.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu121:m123"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
