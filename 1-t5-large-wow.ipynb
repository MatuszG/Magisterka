{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49378c11-ef2a-4887-b1c2-83abd73ce5b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_size 20432\n",
      "valid_dataset 25541\n",
      "max_length 237\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from transformers import TrainingArguments, Trainer, AutoModelForCausalLM, AutoTokenizer, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "output_path = 'Models/t5-base/wow'\n",
    "model_name = \"google-t5/t5-base\"\n",
    "\n",
    "torch.manual_seed(42)\n",
    "texts = pd.read_csv('data_wow.csv')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')\n",
    "valid_dataset = []\n",
    "for sentence in texts['sentence']:\n",
    "    if len(tokenizer.encode(sentence)) < 1024:\n",
    "        valid_dataset.append(sentence)\n",
    "        \n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, txt_list, tokenizer, max_length):\n",
    "        self.labels = []\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []        \n",
    "        for sentence in txt_list:\n",
    "            encodings_dict = tokenizer(sentence, truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
    "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "    def __len__(self): return len(self.input_ids)\n",
    "    def __getitem__(self, idx): return self.input_ids[idx], self.attn_masks[idx]\n",
    "\n",
    "max_length = max([len(tokenizer.encode(sentence)) for sentence in valid_dataset])\n",
    "text_dataset = TextDataset(valid_dataset, tokenizer, max_length=max_length)\n",
    "train_size = int(0.8 * len(valid_dataset))\n",
    "train_dataset, val_dataset = random_split(text_dataset, [train_size, len(text_dataset) - train_size])\n",
    "# print(texts)\n",
    "print('train_size', train_size)\n",
    "print('valid_dataset', len(valid_dataset))\n",
    "print('max_length', max_length)\n",
    "os.environ[\"WANDB_PROJECT\"]='t5-base-wow'\n",
    "os.environ[\"WANDB_LOG_MODEL\"]=\"true\"\n",
    "os.environ[\"WANDB_WATCH\"]=\"false\"\n",
    "os.environ[\"WANDB_NAME\"]=\"t5-base-wow\"\n",
    "os.environ[\"WANDB_API_KEY\"] = \"b689f7c91f1ec7520fa8da927f175f1efd587181\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbe14f7b-ff65-413a-bd83-51e9a9cfaa4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "try:\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(os.path.join(output_path, 'results', 'checkpoint-511')).cuda() #5621\n",
    "    print('saved')\n",
    "except:\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).cuda()\n",
    "    print('downloaded')\n",
    "\n",
    "\n",
    "# model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11f23b30-ae77-4891-9ad7-a3bd1273f31d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-31 06:02:02,179] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgarbacik-mateusz\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/wandb/run-20240731_060204-7jo3q1kr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/garbacik-mateusz/t5-base-wow/runs/7jo3q1kr' target=\"_blank\">Models/t5-base/wow/results</a></strong> to <a href='https://wandb.ai/garbacik-mateusz/t5-base-wow' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/garbacik-mateusz/t5-base-wow' target=\"_blank\">https://wandb.ai/garbacik-mateusz/t5-base-wow</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/garbacik-mateusz/t5-base-wow/runs/7jo3q1kr' target=\"_blank\">https://wandb.ai/garbacik-mateusz/t5-base-wow/runs/7jo3q1kr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5688' max='12775' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5688/12775 3:40:45 < 4:35:09, 0.43 it/s, Epoch 11.13/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.752600</td>\n",
       "      <td>0.301425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.270500</td>\n",
       "      <td>0.160294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.154000</td>\n",
       "      <td>0.097471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.098000</td>\n",
       "      <td>0.063786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.066400</td>\n",
       "      <td>0.044100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.047100</td>\n",
       "      <td>0.031855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.034700</td>\n",
       "      <td>0.023482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.026200</td>\n",
       "      <td>0.017688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.020100</td>\n",
       "      <td>0.013652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.015700</td>\n",
       "      <td>0.010699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.008471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 27\u001b[0m\n\u001b[1;32m      4\u001b[0m training_args \u001b[38;5;241m=\u001b[39m Seq2SeqTrainingArguments(output_dir\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      5\u001b[0m                                   num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m,\n\u001b[1;32m      6\u001b[0m                                   load_best_model_at_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m                                   logging_dir\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogs\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     16\u001b[0m                                   report_to \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwandb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     19\u001b[0m         args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     20\u001b[0m         train_dataset \u001b[38;5;241m=\u001b[39m train_dataset, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m                                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mstack([f[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m data]),\n\u001b[1;32m     25\u001b[0m                                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mstack([f[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m data])})\n\u001b[0;32m---> 27\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# model.save_pretrained(os.path.join(output_path, 'results'))\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# tokenizer.save_pretrained(os.path.join(output_path, 'results'))\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# add t5 model to training\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# add gpt-2-large \u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1932\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1930\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1931\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1933\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1936\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1937\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2268\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2267\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2268\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2271\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2272\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2273\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2274\u001b[0m ):\n\u001b[1;32m   2275\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2276\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3307\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3306\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3307\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3309\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3311\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3338\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3336\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3337\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3338\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3339\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3340\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1739\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1736\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m-> 1739\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1748\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1749\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1752\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1754\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1756\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1106\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1091\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1092\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39mforward,\n\u001b[1;32m   1093\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1103\u001b[0m         output_attentions,\n\u001b[1;32m   1104\u001b[0m     )\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1106\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:716\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    714\u001b[0m     query_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 716\u001b[0m cross_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    727\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    729\u001b[0m \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:626\u001b[0m, in \u001b[0;36mT5LayerCrossAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, query_length, output_attentions)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    615\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    616\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    625\u001b[0m ):\n\u001b[0;32m--> 626\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    627\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mEncDecAttention(\n\u001b[1;32m    628\u001b[0m         normed_hidden_states,\n\u001b[1;32m    629\u001b[0m         mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    636\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    637\u001b[0m     )\n\u001b[1;32m    638\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:245\u001b[0m, in \u001b[0;36mT5LayerNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# T5 uses a layer_norm which only scales and doesn't shift, which is also known as Root Mean\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# Square Layer Normalization https://arxiv.org/abs/1910.07467 thus varience is calculated\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m# w/o mean and there is no bias. Additionally we want to make sure that the accumulation for\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# half-precision inputs is done in fp32\u001b[39;00m\n\u001b[0;32m--> 245\u001b[0m     variance \u001b[38;5;241m=\u001b[39m \u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrsqrt(variance \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariance_epsilon)\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;66;03m# convert into half-precision if necessary\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(output_dir=os.path.join(output_path, 'results'),\n",
    "                                  num_train_epochs=25,\n",
    "                                  load_best_model_at_end=True,\n",
    "                                  overwrite_output_dir=True,\n",
    "                                  eval_strategy=\"epoch\",\n",
    "                                  save_strategy=\"epoch\",\n",
    "                                  per_device_train_batch_size=20,\n",
    "                                  per_device_eval_batch_size=20,\n",
    "                                  warmup_steps=100,\n",
    "                                  weight_decay=0.03,\n",
    "                                  gradient_accumulation_steps=2,\n",
    "                                  logging_dir=os.path.join(output_path, 'logs'),\n",
    "                                  report_to = 'wandb')\n",
    "\n",
    "trainer = Seq2SeqTrainer(model=model,\n",
    "        args=training_args,\n",
    "        train_dataset = train_dataset, \n",
    "        eval_dataset = val_dataset,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "        data_collator = lambda data: {'input_ids': torch.stack([f[0] for f in data]),\n",
    "                                      'attention_mask': torch.stack([f[1] for f in data]),\n",
    "                                      'labels': torch.stack([f[0] for f in data])})\n",
    "\n",
    "trainer.train()\n",
    "# model.save_pretrained(os.path.join(output_path, 'results'))\n",
    "# tokenizer.save_pretrained(os.path.join(output_path, 'results'))\n",
    "\n",
    "# add t5 model to training\n",
    "# add gpt-2-large "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a06573cf-2fd6-4057-8b36-107fad12b4f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    0, 11029,    10, 22130,  1947,   106,    31,     7,   205,  4207,\n",
      "           10, 22130,  1947,   106,    31,     7,   205,  4207, 11029,    10,\n",
      "        22130,  1947,   106,    31,     7,   205,  4207,    10, 22130,  1947,\n",
      "          106,    31,     7,   205,  4207,    10, 22130,  1947,   106,    31,\n",
      "            7,   205,  4207,    10, 22130,  1947,   106,    31,     7,   205,\n",
      "         4207,    10,   205,  4207,    10, 22130,  1947,   106,    31,     7,\n",
      "          205,  4207,    10,   205,  4207,    10, 22130,  1947,   106,    31,\n",
      "            7,   205,  4207,    10,   205,  4207,    10, 22130,  1947,   106,\n",
      "           31,     7,   205,  4207,    10, 22130,  1947,   106,    31,     7,\n",
      "          205,  4207,    10,   205,  4207,    10, 22130,  1947,   106,    31,\n",
      "            7,   205,  4207,    10,   205,  4207,    10, 22130,  1947,   106,\n",
      "           31,     7,   205,  4207,    10, 22130,  1947,   106,    31,     7,\n",
      "          205,  4207,    10, 22130,  1947,   106,    31,     7,   205,  4207,\n",
      "           10, 22130,  1947,   106,    31,     7,   205,  4207,    10,   205,\n",
      "         4207,    10, 22130,  1947,   106,    31,     7,   205,  4207,    10,\n",
      "        22130,  1947,   106,    31,     7,   205,  4207,    10, 22130,  1947,\n",
      "          106,    31,     7,   205,  4207,    10, 22130,  1947,   106,    31,\n",
      "            7,   205,  4207,    10, 22130,  1947,   106,    31,     7,   205,\n",
      "         4207,    10, 22130,  1947,   106,    31,     7,   205,  4207,    10,\n",
      "        22130,  1947,   106,    31,     7,   205,  4207,    10, 22130,  1947,\n",
      "          106,    31,     7,   205,  4207,    10, 22130,  1947,   106,    31,\n",
      "            7,   205,  4207,    10, 22130,  1947,   106,    31,     7,   205,\n",
      "         4207,    10, 22130,  1947,   106,    31,     7,   205,  4207,    10,\n",
      "        22130,  1947,   106,    31,     7,   205,  4207,    10, 22130,  1947,\n",
      "          106,    31,     7,   205,  4207,    10, 22130,  1947,   106,    31,\n",
      "            7,   205,  4207,    10, 22130,  1947,   106,    31,     7,   205,\n",
      "         4207,    10, 22130,  1947,   106,    31,     7,   205,  4207,    10,\n",
      "        22130,  1947,   106,    31,     7,   205,  4207,    10, 22130,  1947,\n",
      "          106,    31,     7,   205,  4207,    10, 22130,  1947,   106,    31,\n",
      "            7,   205,  4207,    10, 22130,  1947,   106,    31,     7,   205],\n",
      "       device='cuda:0')\n",
      "[\"Title: Sharptalon's Claw: Sharptalon's Claw Title: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's C\", \"Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw\", \"Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw\", \"Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw\", \"Title: Sharptalon's Claw: Sharptalon's Claw Title: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon\", \"Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw:\", \"Title: Sharptalon's Claw: Sharptalon's Claw Title: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's C\", \"Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's C\", \"Title: Sharptalon's Claw: Sharptalon's Claw Title: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon'\", \"Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw\", \"Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw\", \"Title: Sharptalon's Claw: Sharptalon's Claw Title: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharp\", \"Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw\", \"Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw\", \"Title: Sharptalon's Claw: Sharptalon's Claw Title: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's\", \"Title: Sharptalon's Claw: Sharptalon's Claw Title: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's C\", \"Title: Sharptalon's Claw: Sharptalon's Claw Title: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw\", \"Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharp\", \"Title: Sharptalon's Claw: Sharptalon's Claw Title: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw:\", \"Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw\", \"Title: Sharptalon's Claw: Sharptalon's Claw Title: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw\", \"Title: Sharptalon's Claw: Sharptalon's Claw Title: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw\", \"Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw\", \"Title: Sharptalon's Claw: Sharptalon's Claw Title: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Title: Sharptalon's Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw:\", \"Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw\", \"Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw\", \"Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharp\", \"Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw\", \"Title: Sharptalon's Claw: Sharptalon's Claw Title: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharp\", \"Title: Sharptalon's Claw: Sharptalon's Claw Title: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw\", \"Title: Sharptalon's Claw: Sharptalon's Claw Title: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon\", \"Title: Sharptalon's Claw Title: Sharptalon's Claw: Sharptalon's Claw Title: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Claw: Title: Sharptalon's Claw: Claw: Title: Sharptalon's Claw: Sharptalon\", \"Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw\", \"Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw\", \"Title: Sharptalon's Claw Title: Sharptalon's Claw: Sharptalon's Claw Title: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharp\", \"Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw\", \"Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw\", \"Title: Sharptalon's Claw Title: Sharptalon's Claw: Sharptalon's Claw Title: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw:\", \"Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw\", \"Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw\", \"Title: Sharptalon's Claw: Sharptalon's Claw Title: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharp\", \"Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw\", \"Title: Sharptalon's Claw: Sharptalon's Claw Title: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon\", \"Title: Sharptalon's Claw: Sharptalon's Claw Title: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon'\", \"Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw\", \"Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptal\", \"Title: Sharptalon's Claw: Sharptalon's Claw Title: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw\", \"Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw\", \"Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon's Claw: Sharptalon\", \"Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw Title: Sharptalon's Claw\"]\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Title: Sharptalon's Claw\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "model.eval()\n",
    "try:\n",
    "    sample_outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        max_length=300,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7,\n",
    "        num_return_sequences=50\n",
    "    )\n",
    "    print(sample_outputs[0])\n",
    "    # Decode and print generated texts\n",
    "    generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in sample_outputs]\n",
    "    print(generated_texts)\n",
    "    with open(os.path.join(output_path, 'results','output.txt'), 'w') as file:\n",
    "        file.writelines([f\"Generated text {i+1}:\\n{text}\\n\" for i, text in enumerate(generated_texts)])\n",
    "\n",
    "except RuntimeError as e:\n",
    "    print(\"RuntimeError during generation:\", e)\n",
    "\n",
    "    # Additional Debugging: Check logits\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids)\n",
    "        logits = outputs.logits\n",
    "        assert not torch.isnan(logits).any(), \"logits contain NaNs\"\n",
    "        assert not torch.isinf(logits).any(), \"logits contain Infs\"\n",
    "        print(\"Logits sample:\", logits[0, -1, :10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9180eb-0029-44bf-9d89-17de8b9ec1f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu121.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu121:m123"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
